{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77b1c6df",
   "metadata": {},
   "source": [
    "# Sistem Pencarian Dokumen menggunakan TF-IDF dan SBERT\n",
    "## 1. Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6250fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download NLTK resources (only needed once)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('Data/arXiv_scientific dataset.csv')\n",
    "\n",
    "# Use the entire dataset\n",
    "dataset = df.copy().reset_index(drop=True)\n",
    "\n",
    "# Extract title and summary columns\n",
    "papers = dataset[['title', 'summary']].dropna()\n",
    "\n",
    "# Check if category column exists (for evaluation purposes)\n",
    "has_categories = 'category' in dataset.columns\n",
    "if has_categories:\n",
    "    papers['category'] = dataset['category']\n",
    "    print(f\"Dataset has {papers['category'].nunique()} unique categories\")\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by lowercasing, removing punctuation, lemmatizing and removing stopwords\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    # Remove stopwords and apply lemmatization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "# Apply preprocessing to title and summary\n",
    "papers['cleaned_title'] = papers['title'].apply(preprocess_text)\n",
    "papers['cleaned_summary'] = papers['summary'].apply(preprocess_text)\n",
    "\n",
    "# Combine title and summary for better search results\n",
    "papers['combined'] = papers['cleaned_title'] + \" \" + papers['cleaned_summary']\n",
    "\n",
    "# Display information about the dataset\n",
    "print(f\"Dataset loaded: {len(papers)} documents\")\n",
    "print(f\"Preprocessing completed in {time.time() - start_total:.2f} seconds\")\n",
    "print(\"\\nSample document:\")\n",
    "print(papers[['title', 'summary']].iloc[0])\n",
    "print(\"\\nAfter preprocessing (with lemmatization):\")\n",
    "print(papers[['cleaned_title', 'cleaned_summary']].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03c799a",
   "metadata": {},
   "source": [
    "### Memahami Lemmatization\n",
    "\n",
    "**Lemmatization** adalah proses mereduksi kata ke bentuk dasar atau bentuk kamus (yang disebut lemma). Berbeda dengan stemming yang hanya memotong akhiran kata, lemmatization mempertimbangkan konteks dan mengubah kata menjadi bentuk dasar yang bermakna.\n",
    "\n",
    "**Manfaat lemmatization untuk pencarian informasi:**\n",
    "\n",
    "1. **Normalisasi**: Bentuk-bentuk berbeda dari kata yang sama (\"running\", \"runs\", \"ran\") dinormalisasi ke bentuk dasar umum (\"run\")\n",
    "2. **Peningkatan recall**: Kueri akan mencocokkan lebih banyak dokumen relevan terlepas dari variasi kata\n",
    "3. **Makna semantik lebih baik**: Mempertahankan arti kata tidak seperti stemming yang dapat menghasilkan kata yang tidak bermakna\n",
    "4. **Dimensi yang lebih kecil**: Lebih sedikit istilah unik dalam model ruang vektor\n",
    "\n",
    "**Contoh transformasi:**\n",
    "- \"better\" → \"good\"\n",
    "- \"running\" → \"run\" \n",
    "- \"studies\" → \"study\"\n",
    "- \"computing\" → \"compute\"\n",
    "\n",
    "Langkah preprocessing ini akan meningkatkan pendekatan TF-IDF dan SBERT dengan menyediakan data teks yang lebih ternormalisasi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2e58b",
   "metadata": {},
   "source": [
    "## 2. Vektorisasi Dokumen\n",
    "### 2.1 Vektorisasi TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0069b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"Performing TF-IDF vectorization...\")\n",
    "# Measure TF-IDF processing time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize TF-IDF Vectorizer with parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # Limit features to top 5000\n",
    "    stop_words='english',  # Additional stopword removal\n",
    "    ngram_range=(1, 1)  # Use unigrams only\n",
    ")\n",
    "\n",
    "# Generate TF-IDF matrix from preprocessed text\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(papers['combined'])\n",
    "\n",
    "# Record TF-IDF processing time\n",
    "tfidf_time = time.time() - start_time\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape} (documents × features)\")\n",
    "print(f\"TF-IDF vectorization completed in {tfidf_time:.2f} seconds\")\n",
    "\n",
    "# Get some statistics about the TF-IDF matrix\n",
    "print(f\"Sparsity: {100.0 * (1.0 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])):.2f}%\")\n",
    "print(f\"Top 5 features: {list(tfidf_vectorizer.vocabulary_.keys())[:5]}\")\n",
    "\n",
    "# Convert the TF-IDF sparse matrix to a pandas DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Save the TF-IDF matrix to an HDF5 file\n",
    "tfidf_df.to_hdf('tfidf_matrix.h5', key='tfidf', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7410b",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) adalah teknik pembobotan kata yang menunjukkan pentingnya suatu kata dalam dokumen relatif terhadap koleksi dokumen. \n",
    "\n",
    "**Komponen TF-IDF:**\n",
    "1. **Term Frequency (TF)**: Mengukur seberapa sering kata muncul dalam dokumen. Ini menunjukkan relevansi kata terhadap dokumen tersebut.\n",
    "2. **Inverse Document Frequency (IDF)**: Mengukur seberapa penting kata tersebut. Kata yang muncul di banyak dokumen (seperti \"dan\", \"atau\") mendapat nilai IDF rendah.\n",
    "\n",
    "**Keuntungan TF-IDF:**\n",
    "- Mudah diimplementasikan dan diinterpretasi\n",
    "- Efisien secara komputasi\n",
    "- Bekerja dengan baik untuk pencarian berbasis kata kunci\n",
    "\n",
    "**Kekurangan TF-IDF:**\n",
    "- Tidak memahami semantik atau konteks\n",
    "- Tidak mengenali sinonim atau kata terkait\n",
    "- Tergantung pada kecocokan kata yang tepat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5190900b",
   "metadata": {},
   "source": [
    "### 2.2 SBERT (Sentence-BERT) Embeddings\n",
    "\n",
    "SBERT adalah model bahasa berbasis Transformer yang mengubah teks menjadi representasi vektor padat yang menangkap makna semantik. Berbeda dengan TF-IDF yang hanya menghitung frekuensi kata, SBERT memahami konteks, sinonim, dan hubungan semantik lainnya.\n",
    "\n",
    "**Cara Kerja SBERT:**\n",
    "1. Menggunakan model BERT yang telah dilatih sebelumnya untuk menghasilkan embeddings kontekstual\n",
    "2. Dioptimalkan khusus untuk tugas-tugas perbandingan semantik seperti pencarian dokumen\n",
    "3. Mampu menangkap hubungan semantik yang kompleks antar kata dan kalimat\n",
    "\n",
    "**Keuntungan SBERT:**\n",
    "- Pemahaman semantik yang lebih baik\n",
    "- Dapat mengenali sinonim dan konsep terkait\n",
    "- Bekerja dengan baik bahkan ketika kueri menggunakan terminologi yang berbeda\n",
    "\n",
    "**Kekurangan SBERT:**\n",
    "- Membutuhkan lebih banyak sumber daya komputasi\n",
    "- Lebih lambat daripada metode tradisional seperti TF-IDF\n",
    "- Model yang lebih kompleks untuk diimplementasikan dan dioptimalkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"Generating SBERT embeddings...\")\n",
    "# Measure SBERT processing time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load pre-trained SBERT model\n",
    "sbert_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Generate document embeddings\n",
    "try:\n",
    "    # Using convert_to_numpy=True for compatibility with sklearn\n",
    "    sbert_embeddings = sbert_model.encode(\n",
    "        papers['combined'].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    sbert_success = True\n",
    "except Exception as e:\n",
    "    print(f\"Error in SBERT encoding: {e}\")\n",
    "    sbert_embeddings = np.zeros((len(papers), 384))  # Default embedding size\n",
    "    sbert_success = False\n",
    "\n",
    "# Record SBERT processing time\n",
    "sbert_time = time.time() - start_time\n",
    "\n",
    "print(f\"SBERT embeddings shape: {sbert_embeddings.shape} (documents × dimensions)\")\n",
    "print(f\"SBERT encoding completed in {sbert_time:.2f} seconds\")\n",
    "\n",
    "# Convert embeddings to pandas DataFrame\n",
    "embeddings_df = pd.DataFrame(sbert_embeddings)\n",
    "\n",
    "# Save the embeddings in HDF5 format\n",
    "embeddings_df.to_hdf('sbert_embeddings.h5', key='embeddings', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd350eb",
   "metadata": {},
   "source": [
    "## 3. Perhitungan Similaritas\n",
    "### 3.1 Cosine Similarity untuk TF-IDF\n",
    "\n",
    "Cosine Similarity adalah metode untuk mengukur kesamaan antara dua vektor dengan menghitung cosinus sudut di antara keduanya. Nilai berkisar dari -1 (berlawanan arah) hingga 1 (arah yang sama), dengan 0 menunjukkan ketidaksamaan.\n",
    "\n",
    "Dalam konteks pencarian dokumen, cosine similarity digunakan untuk membandingkan representasi vektor dari kueri dengan dokumen. Semakin tinggi nilai cosine similarity, semakin mirip dokumen tersebut dengan kueri.\n",
    "\n",
    "Formula:\n",
    "cos(θ) = (A·B) / (||A||·||B||)\n",
    "\n",
    "Di mana:\n",
    "- A·B adalah produk dot dari vektor A dan B\n",
    "- ||A|| dan ||B|| adalah magnitudo (panjang) dari vektor A dan B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef1b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "print(\"Computing TF-IDF similarity matrix...\")\n",
    "\n",
    "# Use a memory-efficient approach with sampling for TF-IDF\n",
    "sample_size = min(30000, tfidf_matrix.shape[0])  # Limit sample size to avoid memory issues\n",
    "print(f\"Using sample of {sample_size} documents for TF-IDF similarity calculation\")\n",
    "\n",
    "# Take random indices for sampling\n",
    "np.random.seed(42)  # For reproducibility\n",
    "sample_indices = np.random.choice(tfidf_matrix.shape[0], sample_size, replace=False)\n",
    "tfidf_matrix_sample = tfidf_matrix[sample_indices]\n",
    "\n",
    "# Calculate cosine similarity for the sampled matrix\n",
    "start_time = time()\n",
    "cosine_sim_tfidf = cosine_similarity(tfidf_matrix_sample)\n",
    "tfidf_sim_time = time() - start_time\n",
    "\n",
    "# Track total time\n",
    "tfidf_total_time = tfidf_time + tfidf_sim_time\n",
    "\n",
    "print(f\"TF-IDF similarity matrix shape: {cosine_sim_tfidf.shape}\")\n",
    "print(f\"Similarity calculation completed in {tfidf_sim_time:.2f} seconds\")\n",
    "print(f\"Total TF-IDF processing time: {tfidf_total_time:.2f} seconds\")\n",
    "\n",
    "# Display sample similarity scores\n",
    "print(\"\\nSample similarity scores for document #1:\")\n",
    "top_5_similar = cosine_sim_tfidf[0].argsort()[-6:-1][::-1]  # Skip self\n",
    "for idx in top_5_similar:\n",
    "    original_idx = sample_indices[idx]\n",
    "    print(f\"Document #{original_idx}: {cosine_sim_tfidf[0][idx]:.4f} - '{papers['title'].iloc[original_idx][:50]}...'\")\n",
    "\n",
    "# Function to find similar documents for any document index\n",
    "def find_similar_tfidf_docs(doc_index, top_k=5):\n",
    "    \"\"\"Find similar documents using TF-IDF vectors\"\"\"\n",
    "    # Check if document is in sample\n",
    "    sample_idx = np.where(sample_indices == doc_index)[0] if doc_index in sample_indices else []\n",
    "    \n",
    "    if len(sample_idx) > 0:\n",
    "        # If in sample, use precomputed similarities\n",
    "        sample_idx = sample_idx[0]\n",
    "        scores = cosine_sim_tfidf[sample_idx]\n",
    "        top_idxs = scores.argsort()[-top_k-1:-1][::-1]  # Exclude self\n",
    "        return [(sample_indices[i], scores[i]) for i in top_idxs]\n",
    "    else:\n",
    "        # For documents not in sample, compute on-demand\n",
    "        query_vec = tfidf_matrix[doc_index:doc_index+1]\n",
    "        cos_sim = cosine_similarity(query_vec, tfidf_matrix_sample).flatten()\n",
    "        top_idxs = cos_sim.argsort()[-top_k:][::-1]\n",
    "        return [(sample_indices[i], cos_sim[i]) for i in top_idxs]\n",
    "\n",
    "# Demonstrate on-demand similarity search\n",
    "random_doc = np.random.choice(len(papers))\n",
    "print(f\"\\nOn-demand TF-IDF similarity search for document #{random_doc}:\")\n",
    "similar_docs = find_similar_tfidf_docs(random_doc)\n",
    "for idx, score in similar_docs:\n",
    "    print(f\"Document #{idx}: {score:.4f} - '{papers['title'].iloc[idx][:50]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff1325",
   "metadata": {},
   "source": [
    "### 3.2 Cosine Similarity untuk SBERT\n",
    "\n",
    "Untuk embeddings SBERT, kita juga menggunakan cosine similarity untuk mengukur kesamaan semantik antara dokumen. Meskipun metodenya sama dengan yang digunakan pada TF-IDF, hasilnya sering berbeda karena:\n",
    "\n",
    "1. Vektor SBERT menangkap makna semantik, bukan hanya keberadaan kata\n",
    "2. Embeddings SBERT lebih padat dan dimensionalitas lebih rendah dibandingkan dengan matriks TF-IDF yang sparse\n",
    "3. Dokumen dengan kata-kata berbeda namun makna serupa akan memiliki nilai similaritas yang tinggi\n",
    "\n",
    "Perhitungan cosine similarity pada embeddings SBERT memungkinkan pencarian berdasarkan makna, bukan hanya kecocokan kata kunci, sehingga sering menghasilkan hasil pencarian yang lebih relevan secara kontekstual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306dbe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing SBERT similarity matrix...\")\n",
    "\n",
    "if sbert_success:\n",
    "    # Use the same sampling approach for SBERT to avoid memory issues\n",
    "    sbert_embeddings_sample = sbert_embeddings[sample_indices]\n",
    "    \n",
    "    # Calculate cosine similarity for the sampled matrix\n",
    "    start_time = time()\n",
    "    cosine_sim_sbert = cosine_similarity(sbert_embeddings_sample)\n",
    "    sbert_sim_time = time() - start_time\n",
    "    \n",
    "    # Total SBERT time\n",
    "    sbert_total_time = sbert_time + sbert_sim_time\n",
    "else:\n",
    "    # Fallback if SBERT encoding failed\n",
    "    cosine_sim_sbert = np.eye(len(sample_indices))\n",
    "    sbert_sim_time = 0\n",
    "    sbert_total_time = sbert_time\n",
    "    print(\"Using identity matrix as fallback due to SBERT encoding failure\")\n",
    "\n",
    "print(f\"SBERT similarity matrix shape: {cosine_sim_sbert.shape}\")\n",
    "print(f\"Similarity calculation completed in {sbert_sim_time:.2f} seconds\")\n",
    "print(f\"Total SBERT processing time: {sbert_total_time:.2f} seconds\")\n",
    "\n",
    "# Sample similarity scores for the first document\n",
    "print(\"\\nSample similarity scores for document #1:\")\n",
    "top_5_similar = cosine_sim_sbert[0].argsort()[-6:-1][::-1]  # Skip self\n",
    "for idx in top_5_similar:\n",
    "    original_idx = sample_indices[idx]\n",
    "    print(f\"Document #{original_idx}: {cosine_sim_sbert[0][idx]:.4f} - '{papers['title'].iloc[original_idx][:50]}...'\")\n",
    "\n",
    "# Function to find similar documents using SBERT\n",
    "def find_similar_sbert_docs(doc_index, top_k=5):\n",
    "    \"\"\"Find similar documents using SBERT embeddings\"\"\"\n",
    "    # Check if document is in sample\n",
    "    sample_idx = np.where(sample_indices == doc_index)[0] if doc_index in sample_indices else []\n",
    "    \n",
    "    if len(sample_idx) > 0:\n",
    "        # If in sample, use precomputed similarities\n",
    "        sample_idx = sample_idx[0]\n",
    "        scores = cosine_sim_sbert[sample_idx]\n",
    "        top_idxs = scores.argsort()[-top_k-1:-1][::-1]  # Exclude self\n",
    "        return [(sample_indices[i], scores[i]) for i in top_idxs]\n",
    "    else:\n",
    "        # For documents not in sample, compute on-demand\n",
    "        query_vec = sbert_embeddings[doc_index:doc_index+1]\n",
    "        cos_sim = cosine_similarity(query_vec, sbert_embeddings_sample).flatten()\n",
    "        top_idxs = cos_sim.argsort()[-top_k:][::-1]\n",
    "        return [(sample_indices[i], cos_sim[i]) for i in top_idxs]\n",
    "\n",
    "# Demonstrate on-demand similarity search\n",
    "print(f\"\\nOn-demand SBERT similarity search for document #{random_doc}:\")\n",
    "similar_docs = find_similar_sbert_docs(random_doc)\n",
    "for idx, score in similar_docs:\n",
    "    print(f\"Document #{idx}: {score:.4f} - '{papers['title'].iloc[idx][:50]}...'\")\n",
    "\n",
    "# Store timing information for later comparison\n",
    "model_times = {\n",
    "    'TF-IDF': {\n",
    "        'Processing': tfidf_time,\n",
    "        'Similarity': tfidf_sim_time,\n",
    "        'Total': tfidf_total_time\n",
    "    },\n",
    "    'SBERT': {\n",
    "        'Processing': sbert_time,\n",
    "        'Similarity': sbert_sim_time,\n",
    "        'Total': sbert_total_time\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7f2fd",
   "metadata": {},
   "source": [
    "## 4. Evaluasi Performa\n",
    "\n",
    "Untuk mengevaluasi kinerja sistem pencarian dokumen, kita menggunakan beberapa metrik standar dalam information retrieval:\n",
    "\n",
    "1. **Precision**: Proporsi dokumen yang diambil yang relevan dengan kueri.\n",
    "   - Precision = (Dokumen Relevan yang Diambil) / (Total Dokumen yang Diambil)\n",
    "\n",
    "2. **Recall**: Proporsi dokumen relevan yang berhasil diambil dari seluruh dokumen relevan.\n",
    "   - Recall = (Dokumen Relevan yang Diambil) / (Total Dokumen Relevan)\n",
    "\n",
    "3. **Mean Average Precision (MAP)**: Rata-rata precision pada berbagai tingkat recall.\n",
    "\n",
    "4. **Normalized Discounted Cumulative Gain (nDCG)**: Mengukur kualitas peringkat dengan memberi bobot lebih pada dokumen relevan yang muncul di peringkat atas.\n",
    "\n",
    "5. **Runtime**: Waktu pemrosesan yang diperlukan untuk melakukan vektorisasi dan perhitungan similaritas.\n",
    "\n",
    "Kita akan membandingkan pendekatan TF-IDF dan SBERT menggunakan metrik-metrik ini untuk menentukan metode mana yang memberikan hasil pencarian lebih baik dan lebih efisien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb108ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sim_matrix, df, sample_indices, top_k=30):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval performance using standard metrics\n",
    "    \n",
    "    Args:\n",
    "        sim_matrix: Similarity matrix between sampled documents\n",
    "        df: DataFrame containing document information\n",
    "        sample_indices: Indices of documents in the sample\n",
    "        top_k: Number of top documents to consider\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (precision, recall, mean average precision, ndcg)\n",
    "    \"\"\"\n",
    "    precision, recall, ap, ndcg = [], [], [], []\n",
    "    \n",
    "    # Helper function to calculate Discounted Cumulative Gain\n",
    "    def dcg(scores):\n",
    "        return sum(score / np.log2(idx + 2) for idx, score in enumerate(scores))\n",
    "    \n",
    "    # Only evaluate if we have category information\n",
    "    if 'category' not in df.columns:\n",
    "        print(\"No category information available for evaluation\")\n",
    "        return 0, 0, 0, 0\n",
    "    \n",
    "    # Limit evaluation to a reasonable size\n",
    "    eval_size = min(len(sample_indices), 1000)  # Limit to 1000 documents for speed\n",
    "    eval_indices = sample_indices[:eval_size]\n",
    "    \n",
    "    print(f\"Evaluating on {eval_size} documents...\")\n",
    "    \n",
    "    for i, doc_idx in enumerate(range(eval_size)):\n",
    "        # Get the original index in the full dataset\n",
    "        original_idx = sample_indices[doc_idx]\n",
    "        \n",
    "        # Get true category of current document\n",
    "        true_label = df.iloc[original_idx]['category']\n",
    "        \n",
    "        # Get similarity scores for all other documents in sample\n",
    "        sim_scores = list(enumerate(sim_matrix[doc_idx]))\n",
    "        \n",
    "        # Sort by similarity (descending) and take top_k, excluding the document itself\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_k+1]\n",
    "        \n",
    "        # Map indices back to original dataset\n",
    "        predicted_indices = [sample_indices[x[0]] for x in sim_scores]\n",
    "        \n",
    "        # Get categories of retrieved documents\n",
    "        predicted_categories = df.iloc[predicted_indices]['category'].values\n",
    "        \n",
    "        # Relevance is 1 if category matches, 0 otherwise\n",
    "        rel = [1 if label == true_label else 0 for label in predicted_categories]\n",
    "        \n",
    "        # Calculate precision (fraction of retrieved documents that are relevant)\n",
    "        precision.append(np.mean(rel) if rel else 0)\n",
    "        \n",
    "        # Calculate recall (fraction of relevant documents that are retrieved)\n",
    "        # We use sampled data, so this is an approximation\n",
    "        relevant_in_sample = sum([1 for i in sample_indices if df.iloc[i]['category'] == true_label]) - 1\n",
    "        match_count = sum(rel)\n",
    "        recall.append(match_count / relevant_in_sample if relevant_in_sample > 0 else 0)\n",
    "        \n",
    "        # Calculate Average Precision (AP)\n",
    "        if sum(rel) == 0:\n",
    "            ap.append(0)\n",
    "        else:\n",
    "            precisions = [sum(rel[:k+1]) / (k+1) for k in range(len(rel)) if rel[k] == 1]\n",
    "            ap.append(np.mean(precisions) if precisions else 0)\n",
    "        \n",
    "        # Calculate normalized Discounted Cumulative Gain (nDCG)\n",
    "        ideal = sorted(rel, reverse=True)  # Best possible ranking\n",
    "        ndcg.append(dcg(rel) / dcg(ideal) if dcg(ideal) > 0 else 0)\n",
    "        \n",
    "        # Show progress for long evaluations\n",
    "        if eval_size > 100 and i % 100 == 0:\n",
    "            print(f\"  Evaluated {i}/{eval_size} documents...\")\n",
    "    \n",
    "    # Return mean values of all metrics\n",
    "    return np.mean(precision), np.mean(recall), np.mean(ap), np.mean(ndcg)\n",
    "\n",
    "# Print timing comparison\n",
    "print(\"\\nRuntime Comparison (seconds):\")\n",
    "for model, times in model_times.items():\n",
    "    print(f\"{model}:\")\n",
    "    print(f\"  Processing time: {times['Processing']:.2f}\")\n",
    "    print(f\"  Similarity time: {times['Similarity']:.2f}\")\n",
    "    print(f\"  Total time:      {times['Total']:.2f}\")\n",
    "\n",
    "# Skip evaluation if we don't have categories\n",
    "if 'category' not in papers.columns:\n",
    "    print(\"\\nSkipping evaluation metrics as category information is not available.\")\n",
    "    # Create dummy evaluation results\n",
    "    evaluation_results = {\n",
    "        'TF-IDF': {'Precision': 0, 'Recall': 0, 'MAP': 0, 'nDCG': 0, 'Runtime (s)': tfidf_total_time},\n",
    "        'SBERT': {'Precision': 0, 'Recall': 0, 'MAP': 0, 'nDCG': 0, 'Runtime (s)': sbert_total_time}\n",
    "    }\n",
    "else:\n",
    "    print(\"\\nEvaluating retrieval performance...\")\n",
    "    evaluation_results = {}\n",
    "\n",
    "    # Evaluate TF-IDF performance\n",
    "    print(\"\\nEvaluating TF-IDF...\")\n",
    "    start_time = time()\n",
    "    p_tfidf, r_tfidf, m_tfidf, n_tfidf = evaluate(cosine_sim_tfidf, papers, sample_indices, top_k=30)\n",
    "    tfidf_eval_time = time() - start_time\n",
    "    \n",
    "    print(f\"TF-IDF Evaluation results:\")\n",
    "    print(f\"  Precision: {p_tfidf:.4f}\")\n",
    "    print(f\"  Recall: {r_tfidf:.4f}\")\n",
    "    print(f\"  Mean Average Precision (MAP): {m_tfidf:.4f}\")\n",
    "    print(f\"  Normalized DCG: {n_tfidf:.4f}\")\n",
    "    print(f\"  Evaluation time: {tfidf_eval_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate SBERT performance\n",
    "    print(\"\\nEvaluating SBERT...\")\n",
    "    start_time = time()\n",
    "    p_sbert, r_sbert, m_sbert, n_sbert = evaluate(cosine_sim_sbert, papers, sample_indices, top_k=30)\n",
    "    sbert_eval_time = time() - start_time\n",
    "    \n",
    "    print(f\"SBERT Evaluation results:\")\n",
    "    print(f\"  Precision: {p_sbert:.4f}\")\n",
    "    print(f\"  Recall: {r_sbert:.4f}\")\n",
    "    print(f\"  Mean Average Precision (MAP): {m_sbert:.4f}\")\n",
    "    print(f\"  Normalized DCG: {n_sbert:.4f}\")\n",
    "    print(f\"  Evaluation time: {sbert_eval_time:.2f} seconds\")\n",
    "    \n",
    "    # Store evaluation results\n",
    "    evaluation_results = {\n",
    "        'TF-IDF': {\n",
    "            'Precision': p_tfidf,\n",
    "            'Recall': r_tfidf,\n",
    "            'MAP': m_tfidf,\n",
    "            'nDCG': n_tfidf,\n",
    "            'Runtime (s)': tfidf_total_time\n",
    "        },\n",
    "        'SBERT': {\n",
    "            'Precision': p_sbert,\n",
    "            'Recall': r_sbert,\n",
    "            'MAP': m_sbert,\n",
    "            'nDCG': n_sbert,\n",
    "            'Runtime (s)': sbert_total_time\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Create DataFrame for easier comparison\n",
    "results_df = pd.DataFrame(evaluation_results).T\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults Summary:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a17336",
   "metadata": {},
   "source": [
    "## 5. Visualisasi Performa\n",
    "\n",
    "Pada bagian ini, kita membandingkan performa kedua metode (TF-IDF dan SBERT) secara visual menggunakan grafik. Visualisasi akan mencakup:\n",
    "\n",
    "1. **Metrik Evaluasi**: Perbandingan precision, recall, MAP, dan nDCG antara TF-IDF dan SBERT.\n",
    "\n",
    "2. **Waktu Pemrosesan**: Perbandingan waktu yang dibutuhkan oleh kedua metode, dibagi menjadi:\n",
    "   - Waktu vektorisasi/encoding\n",
    "   - Waktu perhitungan similaritas\n",
    "   - Total waktu pemrosesan\n",
    "\n",
    "Visualisasi ini membantu kita memahami trade-off antara kualitas hasil pencarian dan efisiensi komputasi untuk kedua metode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set better visual style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Create figure for comparison of evaluation metrics\n",
    "metrics = ['Precision', 'Recall', 'MAP', 'nDCG']\n",
    "models = list(evaluation_results.keys())\n",
    "\n",
    "# Create subplots for metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each metric\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [evaluation_results[model][metric] for model in models]\n",
    "    \n",
    "    # Create bar chart\n",
    "    bars = axes[i].bar(models, values, color=['#3274A1', '#E1812C'])\n",
    "    axes[i].set_title(f'Perbandingan {metric}', fontsize=14)\n",
    "    axes[i].set_ylabel(metric)\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[i].text(\n",
    "            bar.get_x() + bar.get_width()/2., \n",
    "            height + 0.01, \n",
    "            f'{height:.4f}', \n",
    "            ha='center', \n",
    "            va='bottom'\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create figure for processing time comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Prepare data for grouped bar chart\n",
    "runtime_data = {\n",
    "    'Processing': [model_times[model]['Processing'] for model in models],\n",
    "    'Similarity': [model_times[model]['Similarity'] for model in models]\n",
    "}\n",
    "\n",
    "# Create positions for bars\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "# Plot grouped bars\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "processing_bars = ax.bar(x - width/2, runtime_data['Processing'], width, \n",
    "                         label='Processing Time', color='#3274A1')\n",
    "similarity_bars = ax.bar(x + width/2, runtime_data['Similarity'], width, \n",
    "                         label='Similarity Time', color='#E1812C')\n",
    "\n",
    "# Add labels and formatting\n",
    "ax.set_ylabel('Time (seconds)')\n",
    "ax.set_title('Runtime Comparison by Stage', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add total time labels\n",
    "for i, model in enumerate(models):\n",
    "    total = model_times[model]['Total']\n",
    "    ax.text(i, max(runtime_data['Processing'][i], runtime_data['Similarity'][i]) + 0.5, \n",
    "            f'Total: {total:.2f}s', ha='center')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a single bar chart for total runtime comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "total_times = [model_times[model]['Total'] for model in models]\n",
    "bars = plt.bar(models, total_times, color=['#3274A1', '#E1812C'])\n",
    "\n",
    "# Add labels and formatting\n",
    "plt.title('Total Runtime Comparison', fontsize=14)\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2., \n",
    "        height * 0.95, \n",
    "        f'{height:.2f}s', \n",
    "        ha='center', \n",
    "        va='bottom',\n",
    "        color='white',\n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db25aec",
   "metadata": {},
   "source": [
    "## 6. Aplikasi Pencarian Dokumen\n",
    "\n",
    "Aplikasi pencarian dokumen ini memungkinkan pengguna untuk mencari paper ilmiah berdasarkan kueri teks. Sistem akan memproses kueri menggunakan kedua metode (TF-IDF dan SBERT) dan menampilkan hasil pencarian berdasarkan similaritas.\n",
    "\n",
    "Langkah-langkah pencarian:\n",
    "1. Kueri pengguna di-preprocess (lowercase, hapus tanda baca, lemmatization, dll.)\n",
    "2. Kueri yang sudah diproses diubah menjadi vektor menggunakan TF-IDF dan SBERT\n",
    "3. Sistem menghitung similaritas antara vektor kueri dan seluruh dokumen dalam dataset\n",
    "4. Dokumen dengan nilai similaritas tertinggi ditampilkan sebagai hasil pencarian\n",
    "\n",
    "Untuk setiap dokumen hasil pencarian, sistem menampilkan:\n",
    "- Judul paper\n",
    "- Ringkasan (sebagian)\n",
    "- Skor similaritas dengan kueri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b71594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document search function\n",
    "def search_documents(query, top_k=10):\n",
    "    \"\"\"\n",
    "    Search for documents using both TF-IDF and SBERT methods\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        top_k: Number of top results to return\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results from both methods\n",
    "    \"\"\"\n",
    "    # Preprocess the query (including lemmatization)\n",
    "    query_preprocessed = preprocess_text(query)\n",
    "    print(f\"Original query: '{query}'\")\n",
    "    print(f\"Preprocessed query: '{query_preprocessed}'\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # TF-IDF search\n",
    "    start_time = time()\n",
    "    query_vec_tfidf = tfidf_vectorizer.transform([query_preprocessed])\n",
    "    \n",
    "    # For searching, we can calculate similarity with all documents\n",
    "    # But do it in batches to avoid memory issues\n",
    "    batch_size = 10000\n",
    "    all_scores_tfidf = []\n",
    "    \n",
    "    for i in range(0, tfidf_matrix.shape[0], batch_size):\n",
    "        end = min(i + batch_size, tfidf_matrix.shape[0])\n",
    "        batch = tfidf_matrix[i:end]\n",
    "        batch_scores = cosine_similarity(query_vec_tfidf, batch).flatten()\n",
    "        all_scores_tfidf.append(batch_scores)\n",
    "    \n",
    "    cos_sim_tfidf = np.concatenate(all_scores_tfidf)\n",
    "    top_indices_tfidf = cos_sim_tfidf.argsort()[-top_k:][::-1]\n",
    "    tfidf_search_time = time() - start_time\n",
    "    \n",
    "    results['TF-IDF'] = {\n",
    "        'time': tfidf_search_time,\n",
    "        'indices': top_indices_tfidf,\n",
    "        'scores': cos_sim_tfidf[top_indices_tfidf],\n",
    "        'titles': papers['title'].iloc[top_indices_tfidf].tolist(),\n",
    "        'summaries': papers['summary'].iloc[top_indices_tfidf].tolist()\n",
    "    }\n",
    "    \n",
    "    # SBERT search\n",
    "    start_time = time()\n",
    "    query_vec_sbert = sbert_model.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # Also batch SBERT similarity calculation\n",
    "    all_scores_sbert = []\n",
    "    \n",
    "    for i in range(0, sbert_embeddings.shape[0], batch_size):\n",
    "        end = min(i + batch_size, sbert_embeddings.shape[0])\n",
    "        batch = sbert_embeddings[i:end]\n",
    "        batch_scores = cosine_similarity(query_vec_sbert, batch).flatten()\n",
    "        all_scores_sbert.append(batch_scores)\n",
    "    \n",
    "    cos_sim_sbert = np.concatenate(all_scores_sbert)\n",
    "    top_indices_sbert = cos_sim_sbert.argsort()[-top_k:][::-1]\n",
    "    sbert_search_time = time() - start_time\n",
    "    \n",
    "    results['SBERT'] = {\n",
    "        'time': sbert_search_time,\n",
    "        'indices': top_indices_sbert,\n",
    "        'scores': cos_sim_sbert[top_indices_sbert],\n",
    "        'titles': papers['title'].iloc[top_indices_sbert].tolist(),\n",
    "        'summaries': papers['summary'].iloc[top_indices_sbert].tolist()\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define query\n",
    "query = \"deep learning for medical diagnosis\"\n",
    "\n",
    "# Perform search\n",
    "print(f\"Searching for: '{query}'\")\n",
    "search_results = search_documents(query, top_k=5)\n",
    "\n",
    "# Display results from both methods\n",
    "for method, results in search_results.items():\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"Top 5 results using {method} (found in {results['time']:.4f} seconds):\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    for i, (idx, score, title, summary) in enumerate(zip(\n",
    "        results['indices'], results['scores'], results['titles'], results['summaries']\n",
    "    )):\n",
    "        print(f\"{i+1}. [{score:.4f}] {title}\")\n",
    "        print(f\"   {summary[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17d894e",
   "metadata": {},
   "source": [
    "## 7. Antarmuka Pencarian Interaktif\n",
    "\n",
    "Antarmuka pencarian interaktif memungkinkan pengguna untuk berinteraksi dengan sistem pencarian melalui komponen GUI sederhana. Pengguna dapat:\n",
    "\n",
    "1. Memasukkan kueri pencarian dalam kotak teks\n",
    "2. Memilih metode pencarian (TF-IDF, SBERT, atau keduanya)\n",
    "3. Menentukan jumlah hasil yang ingin ditampilkan\n",
    "4. Melihat hasil pencarian yang relevan beserta skor similaritas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c21c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Create search interface\n",
    "search_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your search query here...',\n",
    "    description='Search:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "results_count = widgets.Dropdown(\n",
    "    options=[('5 results', 5), ('10 results', 10), ('20 results', 20)],\n",
    "    value=5,\n",
    "    description='Show:',\n",
    ")\n",
    "\n",
    "method_selection = widgets.RadioButtons(\n",
    "    options=['TF-IDF', 'SBERT', 'Both'],\n",
    "    value='Both',\n",
    "    description='Method:',\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    disabled=False,\n",
    "    button_style='primary',\n",
    "    tooltip='Click to search',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_search_button_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        query = search_box.value\n",
    "        \n",
    "        if not query:\n",
    "            print(\"Please enter a search query.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Searching for: '{query}' using {method_selection.value}...\")\n",
    "        results = search_documents(query, top_k=results_count.value)\n",
    "        \n",
    "        if method_selection.value == 'Both':\n",
    "            methods = ['TF-IDF', 'SBERT']\n",
    "        else:\n",
    "            methods = [method_selection.value]\n",
    "        \n",
    "        for method in methods:\n",
    "            print(f\"\\n{'-'*80}\")\n",
    "            print(f\"Top {results_count.value} results using {method} (found in {results[method]['time']:.4f} seconds):\")\n",
    "            print(f\"{'-'*80}\")\n",
    "            \n",
    "            for i, (score, title, summary) in enumerate(zip(\n",
    "                results[method]['scores'], \n",
    "                results[method]['titles'], \n",
    "                results[method]['summaries']\n",
    "            )):\n",
    "                print(f\"{i+1}. [{score:.4f}] {title}\")\n",
    "                print(f\"   {summary[:200]}...\\n\")\n",
    "\n",
    "search_button.on_click(on_search_button_clicked)\n",
    "\n",
    "# Layout the widgets\n",
    "search_layout = widgets.VBox([\n",
    "    widgets.HBox([search_box, search_button]),\n",
    "    widgets.HBox([method_selection, results_count]),\n",
    "    output_area\n",
    "])\n",
    "\n",
    "# Display the interface\n",
    "display(search_layout)\n",
    "\n",
    "print(\"Enter a search query and click 'Search' to find relevant papers.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
